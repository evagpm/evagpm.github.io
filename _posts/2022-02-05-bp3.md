---
layout: post
title: Blog Post 3 - Web Scraping
---

This week we used web scraping to create a recommendation for movies and/or shows to watch based on our favorite.

## Setup

To start I created a github repository to house the project. Then I opened the terminal, navigated to that repository and typed:

```
conda activate PIC16B
scrapy startproject IMDB_scraper
cd IMDB_scraper
```

This used the PIC16B environment and create the necessary files for the project.

## Writing the imdb_spider.py file

Next I created a file inside the spiders directory called imdb_spider.py. At the top, I name the spider and set start_urls to list containing the url for IMDB page for Chicago Fire.

``` python
    name = 'imdb_spider'
    
    #Chicago Fire tv show page
    start_urls = ['https://www.imdb.com/title/tt2261391/']
```

I then created three parsing methods.

### parse

``` python
    """
    The method navigates to the cast and characters page and calls the parse_full_credits

    Inputs: self - an instance of the spider
            response - the page to parse 

    Output: a request to the cast and characters page with parse_full_credits
    """
    def parse(self, response):
        #Combine original url with fullcredits to get full url
        page = response.urljoin("fullcredits")
        yield scrapy.Request(page, callback = self.parse_full_credits)
```

This method is called on start_url when we run the program. It navigates to the page with the list of the cast and crew. We found that the address for this page is just fullcredits after the url for the main page, so we can combine them. Once at that page, it calls the parse_full_credits method.

### parse_full_credits

``` python
    """
    The method scrapes the urls of the case member pages and calls parse_actor_page on each

    Inputs: self - the instance of the spider
            response - the page to parse

    Output: requests to each actors page calling parse_actor_page
    """
    def parse_full_credits(self, response):
        #The links to the actor's page are in the href attribute of the anchors
        #of the table data entries with class primary_photo
        paths = [a.attrib["href"] for a in response.css("td.primary_photo a")]

        #For each actor's relative path
        for path in paths:
            page = response.urljoin(path)

            #Navigate to that page
            yield scrapy.Request(page, callback = self.parse_actor_page)
```

This method parses the fullcredits page to find links to all the actor's pages. Then for each one it creates the full url and navigates to that page, called parse_actor_page. 

### parse_actor_page

``` python
    """
    The method scrapes the name and films for the actor

    Inputs: self - an instance of the spider
            response - the page to parse

    Output: a dict containing the actor and movie name for each film
    """
    def parse_actor_page(self, response):
        #Inside the table data on that the top the text in the first span is the name
        name = response.css("td.name-overview-widget__section span::text")[0].get()

        #The is a div for each film row and within that is a bold anchor with the film title
        films = response.css("div.filmo-row b a::text").getall()

        #For each film
        for film in films:
            #Output a dict with the actor and film name
            yield {
                "Actor": name,
                "Film": film
            }
```

The parse_actor_page function works on each actors page. First it finds the name at the top. Then it gets the film name from each row below. Then for each firm, it yields a dict with the actor name and firm name, which is what will be outputted to the output file.

## Running the spider

Once the files were complete, I returned to the terminal and typed:

```
scrapy crawl imdb_spider -o results.csv
```

This took a while to run, but eventually created results.csv in that folder. This file contains thousands of rows in the format Actor, Film which can be read into a program.

## Processing the Data and Creating the Visualization

I then worked with the data in a jupyter notebook.

### Getting and sorting data

Import the necessary packages.

```python
import pandas as pd
from matplotlib import pyplot as plt  
import seaborn as sns
```

Read in the data from the csv file the spider created.


```python
movies = pd.read_csv("results.csv")
movies.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Actor</th>
      <th>Film</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Taylor Kinney</td>
      <td>Chicago Fire</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Taylor Kinney</td>
      <td>Chicago P.D.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Taylor Kinney</td>
      <td>Chicago Med</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Taylor Kinney</td>
      <td>Here and Now</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Taylor Kinney</td>
      <td>Chicago Justice</td>
    </tr>
  </tbody>
</table>
</div>



Create a counts dict where the keys are the film names and the values are the number of actors from the data table appearing in the film.


```python
counts = dict() #Set up

#For each row of movies
for obs in movies["Film"]:
    #If the film has already been added to counts, add to total
    if obs in counts.keys():
        counts[obs] = counts[obs] + 1
    #Otherwise add it
    else:
        counts[obs] = 1
```

We want to sort the dict by the counts, from biggest to smallest. 


```python
sorted_counts = dict(sorted(counts.items(), key=lambda item: -item[1]))
```

### Creating the chart

For the chart, we'll want the data to be in a list format. I'm also adding another list for if the film is a show in the Chicago franchise, as there's a lot off crossovers there.


```python
films = list(sorted_counts.keys())
counts = list(sorted_counts.values())

#The chicago franchise shows have Chicago as the first 7 characters
franchise = [film[:7] == "Chicago" for film in films]
```

Now we're ready to make a plot of the top thirty films, or rather shows as it turns out.


```python
#Setup a tall plot
fig,ax=plt.subplots(figsize=(6,8))

#Create a barchart of the top thirty
sns.barplot(x = counts[:30], 
            y = films[:30],
            hue = franchise[:30], 
            dodge = False)

#Add a title to the chart
ax.set(title = "Most Common Shows Among Chicago Fire Actors")

#Add a title to the legend
ax.legend(title = "In Chicago Franchise")
```


    
![png](/images/movies.png)
    

As expected, Chicago Fire is number one with all the actors appearing in it. The three other current and past shows in the Franchise are also in the top 14.

## GitHub Repository

You can find the GitHub repository with the code [here](https://github.com/evagpm/blog_post_3).